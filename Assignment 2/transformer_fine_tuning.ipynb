{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Topic Classification\n",
    "\n",
    "The project involves a large dataset of news articles collected over several years. These articles cover a wide range of topics such as world events, sports, business, and science/technology. Each article headline is labeled with a number from 0 to 3, indicating its category, as described below. \n",
    "\n",
    "| Value | Topic        |\n",
    "|:------|:-------------|\n",
    "| 0     | World        |\n",
    "| 1     | Sports       |\n",
    "| 2     | Business     |\n",
    "| 3     | Sci/Tech     |\n",
    "\n",
    "\n",
    "Our goal is to create a model that, given an unknown article headline, can classify it into one of these 4 topics.\n",
    "\n",
    "This specific notebook focuses on fine tuning a tranformer as a way to solve the problem in hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data Set\n",
    "Our dataset consists of only two columns, *text* and *label*, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2\n",
       "5  Stocks End Up, But Near Year Lows (Reuters) Re...      2\n",
       "6  Money Funds Fell in Latest Week (AP) AP - Asse...      2\n",
       "7  Fed minutes show dissent over inflation (USATO...      2\n",
       "8  Safety Net (Forbes.com) Forbes.com - After ear...      2\n",
       "9  Wall St. Bears Claw Back Into the Black  NEW Y...      2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('training_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the train functions\n",
    "###### Analyzing various models to see which one is better off even without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Transformer Model (mrm8488/bert-mini-finetuned-age_news-classification) Accuracy: 0.8\n",
      "Transformer Model (mrm8488/bert-mini-finetuned-age_news-classification) Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [1 4 0 0]\n",
      " [1 0 4 0]\n",
      " [1 0 0 4]]\n",
      "Transformer Model (mrm8488/bert-mini-finetuned-age_news-classification) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.80      0.67         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.84      0.80      0.81        20\n",
      "weighted avg       0.84      0.80      0.81        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load the appropriate model and tokenizer based on the model name.\n",
    "    \"\"\"\n",
    "    if model_name == \"lucasresck/bert-base-cased-ag-news\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    elif model_name == \"fabriceyhc/bert-base-uncased-ag_news\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    else:  # Default to AutoModelForSequenceClassification for other models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def train_data(training_data, test_data):\n",
    "    X_train = training_data['text']\n",
    "    y_train = training_data['label']\n",
    "    X_val = test_data['text']\n",
    "    y_val = test_data['label']\n",
    "\n",
    "    transformer_models = [\n",
    "        \"mrm8488/bert-mini-finetuned-age_news-classification\"\n",
    "    ]\n",
    "\n",
    "    for model_name in transformer_models:\n",
    "        tokenizer, model = load_model_and_tokenizer(model_name)\n",
    "        transformer_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "        label_mapping = {'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n",
    "\n",
    "        transformer_predictions = transformer_pipeline(X_val.tolist())\n",
    "        y_pred_transformer = [label_mapping[pred['label']] for pred in transformer_predictions]\n",
    "        \n",
    "        print('----------------------------------------------------------------')\n",
    "        print(f\"Transformer Model ({model_name}) Accuracy:\", accuracy_score(y_val, y_pred_transformer))\n",
    "        print(f\"Transformer Model ({model_name}) Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_val, y_pred_transformer))\n",
    "        print(f\"Transformer Model ({model_name}) Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred_transformer))\n",
    "\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "\n",
    "train_data(training_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b9863611d94950a98071cf5642eb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c962670f53c844d5a6c75d785e5b4d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263976e72f024f5a8946f8e2b372cbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab4b5e524be4790b351d322ded2f6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26416516304016113, 'eval_runtime': 0.5111, 'eval_samples_per_second': 39.128, 'eval_steps_per_second': 5.869, 'epoch': 1.0}\n",
      "{'loss': 0.1694, 'grad_norm': 0.19779416918754578, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1e5cd1cdd54e6298d1558703ce724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21113094687461853, 'eval_runtime': 0.4213, 'eval_samples_per_second': 47.476, 'eval_steps_per_second': 7.121, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd367a9598aa46d4b2eb3ebd2ce2d46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17189940810203552, 'eval_runtime': 0.4064, 'eval_samples_per_second': 49.211, 'eval_steps_per_second': 7.382, 'epoch': 3.0}\n",
      "{'train_runtime': 1285.4408, 'train_samples_per_second': 4.668, 'train_steps_per_second': 0.583, 'train_loss': 0.15423658752441408, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-mrm8488\\\\tokenizer_config.json',\n",
       " './fine-tuned-mrm8488\\\\special_tokens_map.json',\n",
       " './fine-tuned-mrm8488\\\\vocab.txt',\n",
       " './fine-tuned-mrm8488\\\\added_tokens.json',\n",
       " './fine-tuned-mrm8488\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "train_df = pd.read_csv('training_data.csv')\n",
    "train_df_subset = train_df.sample(n=2000, random_state=42)\n",
    "\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_subset)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "model_name = \"mrm8488/bert-mini-finetuned-age_news-classification\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4) \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    max_length = min(512, max(len(text) for text in examples[\"text\"]))\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./fine-tuned-mrm8488\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-mrm8488\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80         5\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.90      0.90      0.90        20\n",
      "weighted avg       0.90      0.90      0.90        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fine-tuned-mrm8488\")\n",
    "\n",
    "# Define a function to get predictions from the model\n",
    "def get_predictions(model, tokenizer, dataset):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for batch in torch.utils.data.DataLoader(dataset, batch_size=8):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predictions.extend(torch.argmax(outputs.logits, axis=1).tolist())\n",
    "        labels.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "# Get predictions on the test dataset\n",
    "test_predictions, test_labels = get_predictions(model, tokenizer, tokenized_datasets[\"test\"])\n",
    "\n",
    "# Print evaluation metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
