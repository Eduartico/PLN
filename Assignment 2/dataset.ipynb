{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Topic Classification\n",
    "\n",
    "The project involves a large dataset of news articles collected over several years. These articles cover a wide range of topics such as world events, sports, business, and science/technology. Each article headline is labeled with a number from 0 to 3, indicating its category, as described below. \n",
    "\n",
    "| Value | Topic        |\n",
    "|:------|:-------------|\n",
    "| 0     | World        |\n",
    "| 1     | Sports       |\n",
    "| 2     | Business     |\n",
    "| 3     | Sci/Tech     |\n",
    "\n",
    "\n",
    "Our goal is to create a model that, given an unknown article headline, can classify it into one of these 4 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Our Data Set\n",
    "Our dataset consists of only two columns, *text* and *label*, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2\n",
       "5  Stocks End Up, But Near Year Lows (Reuters) Re...      2\n",
       "6  Money Funds Fell in Latest Week (AP) AP - Asse...      2\n",
       "7  Fed minutes show dissent over inflation (USATO...      2\n",
       "8  Safety Net (Forbes.com) Forbes.com - After ear...      2\n",
       "9  Wall St. Bears Claw Back Into the Black  NEW Y...      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('training_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns *text* is, as the named suggests, just the headline of the news article, and *label* has it's classification according to the table mentioned before. As it is obvious, we need to process our text into tokens in order to use it in any prediction models. Here is also a simple code to show that our data is evenly distributed between the four labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    25.0\n",
      "3    25.0\n",
      "1    25.0\n",
      "0    25.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_percentage = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Tokenizing\n",
    "We start by pre-processing our data, as described bellow, and also tokenizing it. Here we create the function **process_data()**, wich receives a dataset, as well as a number to indicate which iteration of the processing we are doing. Later in the notebook it will become clear that more processing is needed, but we didn't want to tamper with the initial results, so we created this way of adding new parts to the function, without making use of them.\n",
    "\n",
    "### Cleaning\n",
    "We start by using a regex to remove any HTML elements, to clean the text data.\n",
    "\n",
    "### Lowercase\n",
    "We then lowercase every word to ensure uniformity and prevent the model from treating words differently based on their capitalization.\n",
    "\n",
    "### Special Characters / Punctuation\n",
    "For this particular problem, special characters and punctuation marks don't contribute much to the meaning of the text and can be removed (opinion could be different if this was a sentiment analysis instead of a topic classification problem)\n",
    "\n",
    "### Tokenization\n",
    "We then split the text into individual words/tokens, as it helps capture semantic more effectively.\n",
    "\n",
    "### Stopwords\n",
    "Stopwords are common words that occur frequently in language, but, in a topic classification problem, carry virtually no useful information. Removing them reduces the noise in the data.\n",
    "\n",
    "### Stemming\n",
    "We reduce words to their base or root form, as it reduces the size of vocabulary and the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Óscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Óscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def tokenize_numbers(text):\n",
    "    year_pattern = r'\\b\\d{4}\\b'\n",
    "    percentage_pattern = r'\\b\\d+(?:\\.\\d+)?%'\n",
    "    time_pattern = r'\\b\\d+\\s*(?:hours?|mins?|minutes?|secs?|seconds?|days?|weeks?|months?|years?|decades?)\\b'\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
    "\n",
    "    text = re.sub(percentage_pattern, 'percentagetoken', text)\n",
    "    text = re.sub(date_pattern, 'datetoken', text)\n",
    "    text = re.sub(year_pattern, 'yeartoken', text)\n",
    "    text = re.sub(time_pattern, 'timetoken', text)\n",
    "    text = re.sub(number_pattern, 'numbertoken', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_rare_words(tokens, threshold=100):\n",
    "    word_counts = Counter(tokens)\n",
    "    return [token for token in tokens if word_counts[token] >= threshold]\n",
    "\n",
    "def process_data(data, mode):\n",
    "    if(mode >= 2):\n",
    "        data['text'] = data['text'].apply(tokenize_numbers)\n",
    "\n",
    "    # Cleaning\n",
    "    data['text'] = data['text'].str.replace(r'#\\d+;', '', regex=True)\n",
    "\n",
    "    # Lowercase\n",
    "    data['text'] = data['text'].str.lower()\n",
    "\n",
    "    # Special Characters / Punctuation\n",
    "    data['text'] = data['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Tokenization\n",
    "    data['tokens'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if(mode >= 2):\n",
    "        stop_words.update(['ap', 'reuters', 'space.com', 'techweb', 'maccentral', 'pc world', 'usatoday.com', 'cnn'])\n",
    "        #data['tokens'] = data['tokens'].apply(remove_rare_words)\n",
    "\n",
    "    data['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "    if(mode == 1 or mode == 2):\n",
    "        data['tokens'] = data['tokens'].apply(stem_tokens)\n",
    "    \n",
    "    if(mode == 3):\n",
    "        data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Now that we can process our data, we can work on training some models. Here we create the function **train_data** to train a few models and compare which yield the best results when classifying the headline topics. We use the the Naive Bayes, the Decision Tree, and the Logistic Regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "def train_data(training_data, test_data, mode):\n",
    "    X_train = training_data['text']\n",
    "    y_train = training_data['label']\n",
    "    X_val = test_data['text']\n",
    "    y_val = test_data['label']\n",
    "\n",
    "    '''\n",
    "    # Initialize the vectorizer\n",
    "    if mode == 3:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word', max_features=6000, lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "    \n",
    "    \n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_val_vect = vectorizer.transform(X_val)\n",
    "    \n",
    "    # Initialize traditional models\n",
    "    model_nb = MultinomialNB()\n",
    "    model_dt = DecisionTreeClassifier(random_state=123)\n",
    "    model_rf = RandomForestClassifier(random_state=123)\n",
    "    model_lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    models = {\n",
    "        \"Naive Bayes\": model_nb,\n",
    "        \"Decision Tree\": model_dt,\n",
    "        \"Random Forest\": model_rf,\n",
    "        \"Logistic Regression\": model_lr\n",
    "    }\n",
    "\n",
    "    # Train and evaluate traditional models\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        y_pred = model.predict(X_val_vect)\n",
    "        print('----------------------------------------------------------------')\n",
    "        print(f\"{model_name} Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "        print(f\"{model_name} Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_val, y_pred))\n",
    "        print(f\"{model_name} Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        print()\n",
    "    \n",
    "    '''\n",
    "    # List of transformer models to evaluate\n",
    "    transformer_models = [\n",
    "        \"textattack/roberta-base-ag-news\",\n",
    "        \"fabriceyhc/bert-base-uncased-ag_news\",\n",
    "        \"lucasresck/bert-base-cased-ag-news\", \n",
    "    ]\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load the appropriate model and tokenizer based on the model name.\n",
    "    \"\"\"\n",
    "    if model_name == \"lucasresck/bert-base-cased-ag-news\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    elif model_name == \"fabriceyhc/bert-base-uncased-ag_news\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    else:  # Default to AutoModelForSequenceClassification for other models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def train_data(training_data, test_data, mode):\n",
    "    X_train = training_data['text']\n",
    "    y_train = training_data['label']\n",
    "    X_val = test_data['text']\n",
    "    y_val = test_data['label']\n",
    "\n",
    "    transformer_models = [\n",
    "        \"textattack/roberta-base-ag-news\",\n",
    "        \"fabriceyhc/bert-base-uncased-ag_news\",\n",
    "        \"lucasresck/bert-base-cased-ag-news\",\n",
    "    ]\n",
    "\n",
    "    for model_name in transformer_models:\n",
    "        tokenizer, model = load_model_and_tokenizer(model_name)\n",
    "        transformer_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "        transformer_predictions = transformer_pipeline(X_val.tolist())\n",
    "        y_pred_transformer = [int(pred['label'].split('_')[-1]) for pred in transformer_predictions]\n",
    "        \n",
    "        print('----------------------------------------------------------------')\n",
    "        print(f\"Transformer Model ({model_name}) Accuracy:\", accuracy_score(y_val, y_pred_transformer))\n",
    "        print(f\"Transformer Model ({model_name}) Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_val, y_pred_transformer))\n",
    "        print(f\"Transformer Model ({model_name}) Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred_transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Óscar\\Desktop\\Projetos\\5º Ano\\PLN\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#training_data = process_data(training_data, mode)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#test_data = process_data(test_data, mode)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 91\u001b[0m, in \u001b[0;36mtrain_data\u001b[1;34m(training_data, test_data, mode)\u001b[0m\n\u001b[0;32m     84\u001b[0m transformer_models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextattack/roberta-base-ag-news\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfabriceyhc/bert-base-uncased-ag_news\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlucasresck/bert-base-cased-ag-news\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m transformer_models:\n\u001b[1;32m---> 91\u001b[0m     tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     transformer_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m     94\u001b[0m     transformer_predictions \u001b[38;5;241m=\u001b[39m transformer_pipeline(X_val\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[45], line 74\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Default to AutoModelForSequenceClassification for other models\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 74\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[1;32mc:\\Users\\Óscar\\Desktop\\Projetos\\5º Ano\\PLN\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1475\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1475\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Óscar\\Desktop\\Projetos\\5º Ano\\PLN\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1463\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1461\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 1\n",
    "\n",
    "#training_data = process_data(training_data, mode)\n",
    "#test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that even though our process has been quite unrefined, with little analysis, we already have promising results, specially the 0,75 Accuracy from Naive Bayes and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1\n",
    "So far we rushed a bit into running our models, but let's take a step back and look at how our tokenized data is coming out of our processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Function to get the top 10 most frequent tokens for each group\n",
    "def get_top_tokens(group):\n",
    "    all_tokens = [token for sublist in group for token in sublist]\n",
    "    token_freq = nltk.FreqDist(all_tokens)\n",
    "    top_tokens = token_freq.most_common(10)\n",
    "    return top_tokens\n",
    "\n",
    "# Group the data by label\n",
    "def print_top_10_from_label(test_data):\n",
    "    grouped_data = test_data.groupby('label')\n",
    "\n",
    "    # Dictionary to store top tokens for each label\n",
    "    top_tokens_dict = {}\n",
    "\n",
    "    # Get top tokens for each label\n",
    "    for label, group in grouped_data:\n",
    "        top_tokens_dict[label] = get_top_tokens(group['tokens'])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, (label, top_tokens) in enumerate(top_tokens_dict.items(), 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        tokens, frequencies = zip(*top_tokens)\n",
    "        plt.bar(tokens, frequencies, color='skyblue')\n",
    "        plt.title(f'Top 10 Tokens for Label: {label}')\n",
    "        plt.xlabel('Tokens')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_top_10_from_label(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see 'AP' and 'Reuters' appearing, and after a little checking we can see that that's left-over text telling us which news source the headline was sampled from, which is only increasing noise in our data, so we add that to the stopwords. We also annalysed the data and found a lot of numbers which are being tokenized as simply the number, and so we created a custom function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-ag-news were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Transformer Model (textattack/roberta-base-ag-news) Accuracy: 0.6\n",
      "Transformer Model (textattack/roberta-base-ag-news) Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [2 1 1 1]\n",
      " [0 0 4 1]\n",
      " [2 0 0 3]]\n",
      "Transformer Model (textattack/roberta-base-ag-news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62         5\n",
      "           1       1.00      0.20      0.33         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.69      0.60      0.57        20\n",
      "weighted avg       0.69      0.60      0.57        20\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Accuracy: 0.7\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [2 2 0 1]\n",
      " [0 0 4 1]\n",
      " [1 0 0 4]]\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.80      0.67         5\n",
      "           1       1.00      0.40      0.57         5\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       0.67      0.80      0.73         5\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.76      0.70      0.69        20\n",
      "weighted avg       0.76      0.70      0.69        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Accuracy: 0.7\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Confusion Matrix:\n",
      "[[3 0 1 1]\n",
      " [3 2 0 0]\n",
      " [0 0 5 0]\n",
      " [1 0 0 4]]\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50         5\n",
      "           1       1.00      0.40      0.57         5\n",
      "           2       0.83      1.00      0.91         5\n",
      "           3       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.77      0.70      0.70        20\n",
      "weighted avg       0.77      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 2\n",
    "\n",
    "#training_data = process_data(training_data, mode)\n",
    "#test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2\n",
    "Here are the same graphs after changes in iteration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_top_10_from_label(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's looking better, let's try changing stemming for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\duart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of the model checkpoint at textattack/roberta-base-ag-news were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Transformer Model (textattack/roberta-base-ag-news) Accuracy: 0.5\n",
      "Transformer Model (textattack/roberta-base-ag-news) Confusion Matrix:\n",
      "[[2 0 1 2]\n",
      " [0 2 0 3]\n",
      " [0 0 3 2]\n",
      " [2 0 0 3]]\n",
      "Transformer Model (textattack/roberta-base-ag-news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.44         5\n",
      "           1       1.00      0.40      0.57         5\n",
      "           2       0.75      0.60      0.67         5\n",
      "           3       0.30      0.60      0.40         5\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.64      0.50      0.52        20\n",
      "weighted avg       0.64      0.50      0.52        20\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Accuracy: 0.6\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Confusion Matrix:\n",
      "[[3 0 2 0]\n",
      " [2 3 0 0]\n",
      " [1 0 3 1]\n",
      " [2 0 0 3]]\n",
      "Transformer Model (fabriceyhc/bert-base-uncased-ag_news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.60      0.46         5\n",
      "           1       1.00      0.60      0.75         5\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.68      0.60      0.62        20\n",
      "weighted avg       0.68      0.60      0.62        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Accuracy: 0.8\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Confusion Matrix:\n",
      "[[3 0 1 1]\n",
      " [0 4 0 1]\n",
      " [0 0 4 1]\n",
      " [0 0 0 5]]\n",
      "Transformer Model (lucasresck/bert-base-cased-ag-news) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.80      0.80      0.80         5\n",
      "           3       0.62      1.00      0.77         5\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.86      0.80      0.80        20\n",
      "weighted avg       0.86      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 3\n",
    "\n",
    "training_data = process_data(training_data, mode)\n",
    "test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
