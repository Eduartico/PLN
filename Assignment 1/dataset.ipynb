{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "### Cleaning\n",
    "We start by using a regex to remove any HTML elements, to clean the text data.\n",
    "\n",
    "### Lowercase\n",
    "We then lowercase every word to ensure uniformity and prevent the model from treating words differently based on their capitalization.\n",
    "\n",
    "### Special Characters / Punctuation\n",
    "For this particular problem, special characters and punctuation marks don't contribute much to the meaning of the text and can be removed (opinion could be different if this was a sentiment analysis instead of a topic classification problem)\n",
    "\n",
    "### Tokenization\n",
    "We then split the text into individual words/tokens, as it helps capture semantic more effectively.\n",
    "\n",
    "### Stopwords\n",
    "Stopwords are common words that occur frequently in language, but, in a topic classification problem, carry virtually no useful information. Removing them reduces the noise in the data.\n",
    "\n",
    "### Stemming\n",
    "We reduce words to their base or root form, as it reduces the size of vocabulary and the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def tokenize_numbers(text):\n",
    "    year_pattern = r'\\b\\d{4}\\b'\n",
    "    percentage_pattern = r'\\b\\d+(?:\\.\\d+)?%'\n",
    "    time_pattern = r'\\b\\d+\\s*(?:hours?|mins?|minutes?|secs?|seconds?|days?|weeks?|months?|years?|decades?)\\b'\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'  # Matches dates in mm/dd/yyyy and dd/mm/yyyy formats\n",
    "\n",
    "    # Tokenize numbers based on patterns\n",
    "    text = re.sub(percentage_pattern, 'percentagetoken', text)\n",
    "    text = re.sub(date_pattern, 'datetoken', text)\n",
    "    text = re.sub(year_pattern, 'yeartoken', text)\n",
    "    text = re.sub(time_pattern, 'timetoken', text)\n",
    "    text = re.sub(number_pattern, 'numbertoken', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_data(data, type):\n",
    "    if(type >= 2):\n",
    "        data['text'] = data['text'].apply(tokenize_numbers)\n",
    "\n",
    "    # Cleaning\n",
    "    data['text'] = data['text'].str.replace(r'#\\d+;', '', regex=True)\n",
    "\n",
    "    # Lowercase\n",
    "    data['text'] = data['text'].str.lower()\n",
    "\n",
    "    # Special Characters / Punctuation\n",
    "    data['text'] = data['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Tokenization\n",
    "    data['tokens'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if(type >= 2):\n",
    "        stop_words.update(['AP', 'Reuters', 'space.com', 'techweb', 'MacCentral', 'PC World', 'USATODAY.COM', 'CNN'])\n",
    "    data['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "    data['tokens'] = data['tokens'].apply(stem_tokens)\n",
    "    \n",
    "    print(data['tokens'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def train_data(training_data, test_data):\n",
    "    X_train = training_data['text']\n",
    "    y_train = training_data['label']\n",
    "    X_val = test_data['text']\n",
    "    y_val = test_data['label']\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer='word', max_features=5000, lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    X_val = vectorizer.transform(X_val)  # Transform validation data\n",
    "    \n",
    "    model_nb = MultinomialNB()\n",
    "    model_dt = DecisionTreeClassifier(random_state=123)\n",
    "    model_rf = RandomForestClassifier(random_state=123)\n",
    "    model_lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        \n",
    "    print('----------------------------------------------------------------')\n",
    "    model_nb = model_nb.fit(X=X_train, y=y_train)\n",
    "    y_pred_nb = model_nb.predict(X_val)\n",
    "    print(\"Naive Bayes Accuracy:\", accuracy_score(y_val, y_pred_nb))\n",
    "    print(\"Naive Bayes Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_nb))\n",
    "    print(\"Naive Bayes Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_nb))\n",
    "    print()\n",
    "    '''\n",
    "    print('----------------------------------------------------------------')\n",
    "    model_dt = model_dt.fit(X=X_train, y=y_train)\n",
    "    y_pred_dt = model_dt.predict(X_val)\n",
    "    print(\"Decision Tree Accuracy:\", accuracy_score(y_val, y_pred_dt))\n",
    "    print(\"Decision Tree Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_dt))\n",
    "    print(\"Decision Tree Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_dt))\n",
    "    print()\n",
    "    \n",
    "    print('----------------------------------------------------------------')\n",
    "    model_rf = model_rf.fit(X=X_train, y=y_train)\n",
    "    y_pred_rf = model_rf.predict(X_val)\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_val, y_pred_rf))\n",
    "    print(\"Random Forest Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_rf))\n",
    "    print(\"Random Forest Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_rf))\n",
    "    print()\n",
    "    '''\n",
    "    print('----------------------------------------------------------------')\n",
    "    model_lr = model_lr.fit(X=X_train, y=y_train)\n",
    "    y_pred_lr = model_lr.predict(X_val)\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_val, y_pred_lr))\n",
    "    print(\"Logistic Regression Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_lr))\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "training_data = process_data(training_data, 2)\n",
    "test_data = process_data(test_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data(training_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
