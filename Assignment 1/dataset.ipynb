{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Topic Classification\n",
    "\n",
    "The project involves a large dataset of news articles collected over several years. These articles cover a wide range of topics such as world events, sports, business, and science/technology. Each article headline is labeled with a number from 0 to 3, indicating its category, as described below. \n",
    "\n",
    "| Value | Topic        |\n",
    "|:------|:-------------|\n",
    "| 0     | World        |\n",
    "| 1     | Sports       |\n",
    "| 2     | Business     |\n",
    "| 3     | Sci/Tech     |\n",
    "\n",
    "\n",
    "Our goal is to create a model that, given an unknown article headline, can classify it into one of these 4 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Our Data Set\n",
    "Our dataset consists of only two columns, *text* and *label*, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2\n",
       "5  Stocks End Up, But Near Year Lows (Reuters) Re...      2\n",
       "6  Money Funds Fell in Latest Week (AP) AP - Asse...      2\n",
       "7  Fed minutes show dissent over inflation (USATO...      2\n",
       "8  Safety Net (Forbes.com) Forbes.com - After ear...      2\n",
       "9  Wall St. Bears Claw Back Into the Black  NEW Y...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('training_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns *text* is, as the named suggests, just the headline of the news article, and *label* has it's classification according to the table mentioned before. As it is obvious, we need to process our text into tokens in order to use it in any prediction models. Here is also a simple code to show that our data is evenly distributed between the four labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    25.0\n",
      "3    25.0\n",
      "1    25.0\n",
      "0    25.0\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_percentage = df['label'].value_counts(normalize=True) * 100\n",
    "print(label_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Tokenizing\n",
    "We start by pre-processing our data, as described bellow, and also tokenizing it. Here we create the function **process_data()**, wich receives a dataset, as well as a number to indicate which iteration of the processing we are doing. Later in the notebook it will become clear that more processing is needed, but we didn't want to tamper with the initial results, so we created this way of adding new parts to the function, without making use of them.\n",
    "\n",
    "### Cleaning\n",
    "We start by using a regex to remove any HTML elements, to clean the text data.\n",
    "\n",
    "### Lowercase\n",
    "We then lowercase every word to ensure uniformity and prevent the model from treating words differently based on their capitalization.\n",
    "\n",
    "### Special Characters / Punctuation\n",
    "For this particular problem, special characters and punctuation marks don't contribute much to the meaning of the text and can be removed (opinion could be different if this was a sentiment analysis instead of a topic classification problem)\n",
    "\n",
    "### Tokenization\n",
    "We then split the text into individual words/tokens, as it helps capture semantic more effectively.\n",
    "\n",
    "### Stopwords\n",
    "Stopwords are common words that occur frequently in language, but, in a topic classification problem, carry virtually no useful information. Removing them reduces the noise in the data.\n",
    "\n",
    "### Stemming\n",
    "We reduce words to their base or root form, as it reduces the size of vocabulary and the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def tokenize_numbers(text):\n",
    "    year_pattern = r'\\b\\d{4}\\b'\n",
    "    percentage_pattern = r'\\b\\d+(?:\\.\\d+)?%'\n",
    "    time_pattern = r'\\b\\d+\\s*(?:hours?|mins?|minutes?|secs?|seconds?|days?|weeks?|months?|years?|decades?)\\b'\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
    "\n",
    "    text = re.sub(percentage_pattern, 'percentagetoken', text)\n",
    "    text = re.sub(date_pattern, 'datetoken', text)\n",
    "    text = re.sub(year_pattern, 'yeartoken', text)\n",
    "    text = re.sub(time_pattern, 'timetoken', text)\n",
    "    text = re.sub(number_pattern, 'numbertoken', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_rare_words(tokens, threshold=200):\n",
    "    word_counts = Counter(tokens)\n",
    "    return [token for token in tokens if word_counts[token] >= threshold]\n",
    "\n",
    "def process_data(data, mode):\n",
    "    if(mode >= 2):\n",
    "        data['text'] = data['text'].apply(tokenize_numbers)\n",
    "\n",
    "    # Cleaning\n",
    "    data['text'] = data['text'].str.replace(r'#\\d+;', '', regex=True)\n",
    "\n",
    "    # Lowercase\n",
    "    data['text'] = data['text'].str.lower()\n",
    "\n",
    "    # Special Characters / Punctuation\n",
    "    data['text'] = data['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Tokenization\n",
    "    data['tokens'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if(mode >= 2):\n",
    "        stop_words.update(['AP', 'Reuters', 'space.com', 'techweb', 'MacCentral', 'PC World', 'USATODAY.COM', 'CNN'])\n",
    "        data['tokens'] = data['tokens'].apply(remove_rare_words)\n",
    "\n",
    "    data['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "    if(mode == 1):\n",
    "        data['tokens'] = data['tokens'].apply(stem_tokens)\n",
    "    \n",
    "    if(mode == 3):\n",
    "        data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Now taht we can process our data, we can work on training some models. Here we create the function **train_data** to train a few models and compare which yield the best results when classifying the headline topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_data(training_data, test_data, mode):\n",
    "    X_train = training_data['text']\n",
    "    y_train = training_data['label']\n",
    "    X_val = test_data['text']\n",
    "    y_val = test_data['label']\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer='word', max_features=6000, lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "    if(mode == 3):\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    X_val = vectorizer.transform(X_val) \n",
    "    \n",
    "    model_nb = MultinomialNB()\n",
    "    model_dt = DecisionTreeClassifier(random_state=123)\n",
    "    model_rf = RandomForestClassifier(random_state=123)\n",
    "    model_lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        \n",
    "    print('----------------------------------------------------------------')\n",
    "    model_nb = model_nb.fit(X=X_train, y=y_train)\n",
    "    y_pred_nb = model_nb.predict(X_val)\n",
    "    print(\"Naive Bayes Accuracy:\", accuracy_score(y_val, y_pred_nb))\n",
    "    print(\"Naive Bayes Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_nb))\n",
    "    print(\"Naive Bayes Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_nb))\n",
    "    print()\n",
    "    \n",
    "    print('----------------------------------------------------------------')\n",
    "    model_dt = model_dt.fit(X=X_train, y=y_train)\n",
    "    y_pred_dt = model_dt.predict(X_val)\n",
    "    print(\"Decision Tree Accuracy:\", accuracy_score(y_val, y_pred_dt))\n",
    "    print(\"Decision Tree Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_dt))\n",
    "    print(\"Decision Tree Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_dt))\n",
    "    print()\n",
    "    \n",
    "    print('----------------------------------------------------------------')\n",
    "    model_lr = model_lr.fit(X=X_train, y=y_train)\n",
    "    y_pred_lr = model_lr.predict(X_val)\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_val, y_pred_lr))\n",
    "    print(\"Logistic Regression Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred_lr))\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Naive Bayes Accuracy: 0.75\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [0 5 0 0]\n",
      " [1 0 4 0]\n",
      " [1 1 1 2]]\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.79      0.75      0.73        20\n",
      "weighted avg       0.79      0.75      0.73        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Decision Tree Accuracy: 0.5\n",
      "Decision Tree Confusion Matrix:\n",
      "[[2 1 1 1]\n",
      " [0 3 0 2]\n",
      " [0 0 3 2]\n",
      " [0 2 1 2]]\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.50      0.60      0.55         5\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.29      0.40      0.33         5\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.60      0.50      0.51        20\n",
      "weighted avg       0.60      0.50      0.51        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Random Forest Accuracy: 0.65\n",
      "Random Forest Confusion Matrix:\n",
      "[[3 0 1 1]\n",
      " [0 3 0 2]\n",
      " [1 0 4 0]\n",
      " [0 1 1 3]]\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.75      0.60      0.67         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       0.50      0.60      0.55         5\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.67      0.65      0.65        20\n",
      "weighted avg       0.67      0.65      0.65        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Logistic Regression Accuracy: 0.75\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[2 1 1 1]\n",
      " [0 4 1 0]\n",
      " [0 0 4 1]\n",
      " [0 0 0 5]]\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.80      0.80      0.80         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.80      0.75      0.73        20\n",
      "weighted avg       0.80      0.75      0.73        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 1\n",
    "\n",
    "training_data = process_data(training_data, mode)\n",
    "test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Naive Bayes Accuracy: 0.75\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [0 5 0 0]\n",
      " [1 0 4 0]\n",
      " [1 1 1 2]]\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.79      0.75      0.73        20\n",
      "weighted avg       0.79      0.75      0.73        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Decision Tree Accuracy: 0.45\n",
      "Decision Tree Confusion Matrix:\n",
      "[[2 0 1 2]\n",
      " [2 2 0 1]\n",
      " [0 0 3 2]\n",
      " [1 1 1 2]]\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.40      0.40         5\n",
      "           1       0.67      0.40      0.50         5\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.29      0.40      0.33         5\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.49      0.45      0.46        20\n",
      "weighted avg       0.49      0.45      0.46        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Random Forest Accuracy: 0.5\n",
      "Random Forest Confusion Matrix:\n",
      "[[1 0 1 3]\n",
      " [0 3 0 2]\n",
      " [1 0 3 1]\n",
      " [0 1 1 3]]\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29         5\n",
      "           1       0.75      0.60      0.67         5\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.33      0.60      0.43         5\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.55      0.50      0.50        20\n",
      "weighted avg       0.55      0.50      0.50        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Logistic Regression Accuracy: 0.75\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[3 1 1 0]\n",
      " [1 3 1 0]\n",
      " [0 0 4 1]\n",
      " [0 0 0 5]]\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.75      0.60      0.67         5\n",
      "           2       0.67      0.80      0.73         5\n",
      "           3       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.75      0.75      0.74        20\n",
      "weighted avg       0.75      0.75      0.74        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 2\n",
    "\n",
    "training_data = process_data(training_data, mode)\n",
    "test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Naive Bayes Accuracy: 0.7\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[4 0 1 0]\n",
      " [0 4 1 0]\n",
      " [1 0 3 1]\n",
      " [1 1 0 3]]\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.80      0.80      0.80         5\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.70      0.70      0.70        20\n",
      "weighted avg       0.70      0.70      0.70        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Decision Tree Accuracy: 0.55\n",
      "Decision Tree Confusion Matrix:\n",
      "[[2 1 0 2]\n",
      " [1 2 0 2]\n",
      " [0 0 3 2]\n",
      " [0 1 0 4]]\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50         5\n",
      "           1       0.50      0.40      0.44         5\n",
      "           2       1.00      0.60      0.75         5\n",
      "           3       0.40      0.80      0.53         5\n",
      "\n",
      "    accuracy                           0.55        20\n",
      "   macro avg       0.64      0.55      0.56        20\n",
      "weighted avg       0.64      0.55      0.56        20\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m training_data \u001b[38;5;241m=\u001b[39m process_data(training_data, mode)\n\u001b[0;32m      8\u001b[0m test_data \u001b[38;5;241m=\u001b[39m process_data(test_data, mode)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain_data\u001b[1;34m(training_data, test_data, mode)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m model_rf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m model_rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_val, y_pred_rf))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('training_data.csv')\n",
    "#training_data = pd.read_csv('training_data_xs.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "mode = 3\n",
    "\n",
    "training_data = process_data(training_data, mode)\n",
    "test_data = process_data(test_data, mode)\n",
    "train_data(training_data, test_data, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
