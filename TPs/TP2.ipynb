{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GaX95nnkQbu"
   },
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/) by Mohd Sanad Zaki Rizvi.\n",
    "\n",
    "# N-GRAM LANGUAGE MODELS\n",
    "\n",
    "N-gram language models are based on computing probabilities for the occurrence of each word given *n-1* previous words.\n",
    "\n",
    "To \"train\" such models, we will make use of the [Reuters](https://www.nltk.org/book/ch02.html) corpus, which contains 10,788 news documents in a total of 1.3 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1645451765839,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "HqoClhOHkQb2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of sentences there are in the corpus. Each sentence is a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\duart\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54716\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPAN RIFT Mounting trade friction between the U . S . And Japan has raised fears among many of Asia ' s exporting nations that the row could inflict far - reaching economic damage , businessmen and officials said . "
     ]
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "print(len(reuters.sents()))\n",
    "\n",
    "print(reuters.sents()[0])\n",
    "for w in reuters.sents()[0]:\n",
    "    print(w, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSbkDouwkQb4"
   },
   "source": [
    "## Unigram model\n",
    "\n",
    "For starters, let's build a unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1645451769412,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "VaLWQjxmkQb5",
    "outputId": "67ec3ca8-4982-46c7-a1a2-3cba411b8956"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "uni_model = defaultdict(int)\n",
    "\n",
    "# Count the frequency of each token\n",
    "for sentence in reuters.sents():\n",
    "    for w in sentence:\n",
    "        uni_model[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the counts, we need to transform them into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = float(sum(uni_model.values()))\n",
    "for w in uni_model:\n",
    "    uni_model[w] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely words\n",
    "\n",
    "How likely is the word 'the'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of 'the' is: 0.033849\n"
     ]
    }
   ],
   "source": [
    "probability_of_the = uni_model['the']\n",
    "print(f\"The probability of 'the' is: {probability_of_the:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most likely word in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely word is '.' with a probability of: 0.055031\n"
     ]
    }
   ],
   "source": [
    "most_likely_word = max(uni_model, key=uni_model.get)\n",
    "probability_of_most_likely_word = uni_model[most_likely_word]\n",
    "\n",
    "print(f\"The most likely word is '{most_likely_word}' with a probability of: {probability_of_most_likely_word:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Based on this unigram language model, we can try generating some text. It will not be pretty, though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-IsAHfWikQb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year supported health included omitted Hicks regulators new name Susumu 10 books which Reuters 1 5 announced 57 in ) two ECUADOR in mln ; through agreement disclosed and Farm CANADA 000 to he last 12 less have officials Senate cents 004 said 24 bushel .\" 266 to for Reed said a a surfaced & produced 8 Market said two Miraflores units aflatoxin / will Kahn lt documents too line 000 claims > rise in Foothill mln PA this ALITALIA 11 or a\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# number of words to generate\n",
    "total_words = 100\n",
    "text = []\n",
    "\n",
    "for i in range(total_words):\n",
    "    # select a random probability threshold\n",
    "    r = random.random() #+ 0.2\n",
    "    # select word above the probability threshold\n",
    "    accumulator = .0\n",
    "    for word in uni_model.keys():\n",
    "        accumulator += uni_model[word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print (' '.join([t for t in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNyUfDqNkQb7"
   },
   "source": [
    "## Bigram model\n",
    "\n",
    "In a bigram model, we'll compute the probability of each word given the previous word as context. To obtain bigrams, we can use NLTK's [bigrams](https://www.nltk.org/_modules/nltk/util.html#bigrams). When doing so, we can padd the input left and right and define our own sequence start and sequence end symbols.\n",
    "\n",
    "We first need to obtain the counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HOIR6wVBkQb7"
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        bi_model[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to transform counts into probabilities. For that, we divide each count by the total number of occurrences of the first word in the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1 in bi_model:\n",
    "    total_count = float(sum(bi_model[w1].values()))\n",
    "    for w2 in bi_model[w1]:\n",
    "        bi_model[w1][w2] /= total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely pairs\n",
    "\n",
    "What are the probabilities of each word following 'today'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of . following \"today\" is: 0.186364\n",
      "The probability of between following \"today\" is: 0.001515\n",
      "The probability of the following \"today\" is: 0.013636\n",
      "The probability of many following \"today\" is: 0.000758\n",
      "The probability of of following \"today\" is: 0.009848\n",
      "The probability of ' following \"today\" is: 0.106818\n",
      "The probability of that following \"today\" is: 0.033333\n",
      "The probability of , following \"today\" is: 0.163636\n",
      "The probability of and following \"today\" is: 0.025000\n",
      "The probability of said following \"today\" is: 0.015909\n"
     ]
    }
   ],
   "source": [
    "printed_words = 0\n",
    "for w in bi_model:\n",
    "    if printed_words < 10:\n",
    "        probability_of_word = bi_model[\"today\"][w]\n",
    "        if (probability_of_word > 0):\n",
    "            printed_words += 1\n",
    "            print(f'The probability of {w} following \"today\" is: {probability_of_word:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the probabilities for sentence-starting words? What do most of them have in common? (Hint: check the *left_pad_symbol* defined above for collecting bigrams.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of ASIAN starting a sentence is: 0.000073\n",
      "The probability of EXPORTERS starting a sentence is: 0.000640\n",
      "The probability of U starting a sentence is: 0.015827\n",
      "The probability of . starting a sentence is: 0.000091\n",
      "The probability of S starting a sentence is: 0.000621\n",
      "The probability of JAPAN starting a sentence is: 0.002997\n",
      "The probability of trade starting a sentence is: 0.000018\n",
      "The probability of between starting a sentence is: 0.000018\n",
      "The probability of the starting a sentence is: 0.000201\n",
      "The probability of And starting a sentence is: 0.001078\n",
      "The probability of Japan starting a sentence is: 0.002029\n",
      "The probability of raised starting a sentence is: 0.000018\n",
      "The probability of of starting a sentence is: 0.000037\n",
      "The probability of Asia starting a sentence is: 0.000018\n",
      "The probability of ' starting a sentence is: 0.000037\n",
      "The probability of that starting a sentence is: 0.000018\n",
      "The probability of - starting a sentence is: 0.000950\n",
      "The probability of and starting a sentence is: 0.000146\n",
      "The probability of said starting a sentence is: 0.000219\n",
      "The probability of They starting a sentence is: 0.008151\n"
     ]
    }
   ],
   "source": [
    "printed_words = 0\n",
    "for w in bi_model:\n",
    "    if printed_words < 20:\n",
    "        probability_of_start = bi_model[\"<s>\"][w]\n",
    "        if (probability_of_start > 0):\n",
    "            printed_words += 1\n",
    "            print(f'The probability of {w} starting a sentence is: {probability_of_start:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Now that we have a bigram model, we can generate text based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "xRXR0uxHkQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The change from April to produce over the company . 25 dlrs vs 11 . </s>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "total_words = 100\n",
    "text = [\"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "    accumulator = 0.0\n",
    "    next_word = None\n",
    "\n",
    "    for word, prob in bi_model[text[-1]].items():\n",
    "        accumulator += prob\n",
    "        if accumulator >= r:\n",
    "            next_word = word\n",
    "            break\n",
    "    \n",
    "    if next_word is None:\n",
    "        next_word = \"</s>\"\n",
    "\n",
    "    text.append(next_word)\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAjueaY4kQb-"
   },
   "source": [
    "## Trigram model\n",
    "\n",
    "In a trigram model, we'll compute the probability of each word given the previous two words as context. To obtain trigrams, we can use NLTK's [trigrams](https://www.nltk.org/_modules/nltk/util.html#trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFpa4uXHkQb_"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely triplets\n",
    "\n",
    "What are the most likely words following \"today the\"?\n",
    "What about \"England has\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt-tvT57kQcA"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the trigram model. Does the generated text start to feel a bit more sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV2b_wVfkQcB"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngWW-8YXkQcC"
   },
   "source": [
    "## N-gram models\n",
    "\n",
    "For larger *n*, we can use NLTK's [n-grams](https://www.nltk.org/_modules/nltk/util.html#ngrams), which allows us to choose an arbitrary *n*.\n",
    "\n",
    "Create your own 4-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDB-aucOkQcC"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely tuples\n",
    "\n",
    "Check the most likely words following \"today the public\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkkHxPTTkQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the 4-gram model. Even better, uh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU3XXz10kQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "language-models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
